{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree (CART)\n",
    "## <a href=\"#I\">I Implementing Decision Tree with Scikit-Learn</a>\n",
    "### <a href=\"#I.1\">I.1 Preparing the Data</a>\n",
    "### <a href=\"#I.2\">I.2 Training the Algorithm</a>\n",
    "### <a href=\"#I.3\">I.3 Making Predictions</a>\n",
    "### <a href=\"#I.4\">I.4 Evaluating the Algorithm</a>\n",
    "### <a href=\"#I.5\">I.5 Advantages and Disadvantages of CART</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree (CART)\n",
    "\n",
    "A decision tree is one of most frequently and widely used __supervised machine learning algorithms__ that can perform both __classification and regression tasks__ (__CART__). <br>\n",
    "\n",
    "The intuition behind the decision tree algorithm is simple, yet very powerful.<br>\n",
    "\n",
    "The basic algorithm used to construct the decision tree is known as the ID3 algorithm. <br>\n",
    "Briefly, the steps to the algorithm are:\n",
    "- Select the best attribute → A out of the dataset \n",
    "- Assign A as the decision attribute for the NODE.\n",
    "- Partition (\"split\") the dataset into 2 subset using the attribute A. \n",
    "- For each subset create a new descendant of the NODE. \n",
    "- If examples are perfectly classified, then STOP else iterate over the new leaf nodes.\n",
    "<br>\n",
    "\n",
    "ID3 consider the \"best attribute\" in terms of which attribute has the most __information gain__, a measure that expresses how well an attribute splits that data into groups based on classification.\n",
    "<br>\n",
    "When you create a Decision Tree object with Scikit learn you have the choice between 2 ways to evaluate the information gain: Gini impurity (gini) and Information Gain Entropy (entropy).\n",
    "\n",
    "ID3 is a greedy algorithm that grows the tree top-down, at each node selecting the attribute that best classifies the local training examples. <br>\n",
    "This process continues until the tree perfectly classifies the training examples or until all attributes have been used.\n",
    "\n",
    "\n",
    "<a id=\"I\"></a>\n",
    "## I Implementing Decision Tree with Scikit-Learn\n",
    "\n",
    "We will be using the __iris dataset__ to build a decision tree __classifier__. The data set contains information of 3 classes of the iris plant with the following attributes: - sepal length - sepal width - petal length - petal width - class: Iris Setosa, Iris Versicolour, Iris Virginica\n",
    "\n",
    "The task is to predict the class of the iris plant based on the attributes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes to predict:  ['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Loading the iris data\n",
    "data = load_iris()\n",
    "print('Classes to predict: ', data.target_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"I.1\"></a>\n",
    "### I.1 Preparing the Data:\n",
    "\n",
    "Preparing the data involves:\n",
    "\n",
    "1. Dividing the data into attributes and labels \n",
    "2. Dividing the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in the data: 150\n"
     ]
    }
   ],
   "source": [
    "#Extracting data attributes\n",
    "X = data.data\n",
    "### Extracting target/ class labels\n",
    "y = data.target\n",
    "print('Number of examples in the data:', X.shape[0])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size = 0.25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"I.2\"></a>\n",
    "### I.2 Training the Algorithm:\n",
    "\n",
    "We have divided the data into training and testing sets. Now is the time to construct our Decision Tree using the training data.<br>\n",
    "Since, this is a classification problem, we will import the __DecisionTreeClassifier__ constructor from the sklearn library.<br> Next, we will set the '__criterion__' to '__entropy__', which sets the measure for splitting the attribute to information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(criterion = 'entropy')\n",
    "#Training the decision tree classifier. \n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"I.3\"></a>\n",
    "### I.3 Making Predictions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting labels on the test set.\n",
    "y_pred =  clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"I.4\"></a>\n",
    "### I.4 Evaluating the Algorithm:\n",
    "\n",
    "__Confusion matrix__, __precision__, __recall__, and __F1 measures__ are the most commonly used metrics for classification tasks. <br>\n",
    "For this case, we will simply __use accuracy_score()__ to calculate the accuracy of the predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score on train data:  1.0\n",
      "Accuracy Score on test data:  0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy Score on train data: ', accuracy_score(y_true=y_train, y_pred=clf.predict(X_train)))\n",
    "print('Accuracy Score on test data: ', accuracy_score(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result it can be observed that the accuracy on the test data is good (> 97%) but we can tune the parameters of the decision tree to try increase the accuracy. <br>\n",
    "The following script will let you play with different parameters and furthermore visualize the corresponding tree.\n",
    "These parameters include: \n",
    "- criterion for evaluating a split (__entropy__ to calculate information gain or __Gini__ impurity), \n",
    "- maximum tree depth, \n",
    "- minimum number of samples required at a leaf node, \n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b6582a4dea4d07886404e27c01fd65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='crit', options=('gini', 'entropy'), value='gini'), Dropdown(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn.datasets import load_iris\n",
    "from IPython.display import SVG\n",
    "from graphviz import Source\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ipywidgets import interactive\n",
    "# load dataset\n",
    "data = load_iris()\n",
    "# feature matrix\n",
    "X = data.data\n",
    "# target vector\n",
    "y = data.target\n",
    "# class labels\n",
    "labels = data.feature_names\n",
    "data.target_names\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size = 0.25)\n",
    "\n",
    "def plot_tree(crit, split, depth, min_split, min_leaf=0.2):\n",
    "    estimator = DecisionTreeClassifier(random_state = 0 \n",
    "                                      , criterion = crit\n",
    "                                      , splitter = split\n",
    "                                      , max_depth = depth\n",
    "                                      , min_samples_split=min_split\n",
    "                                      , min_samples_leaf=min_leaf)\n",
    "    estimator.fit(X_train, y_train)\n",
    "    y_pred =  estimator.predict(X_test)\n",
    "     \n",
    "    print('Accuracy Score on train data: ', accuracy_score(y_true=y_train, y_pred=estimator.predict(X_train)))\n",
    "    print('Accuracy Score on test data: ', accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "    graph = Source(tree.export_graphviz(estimator\n",
    "                                      , out_file=None\n",
    "                                      , feature_names=labels\n",
    "                                      , class_names=['0', '1', '2']\n",
    "                                      , filled = True))\n",
    "\n",
    "    display(SVG(graph.pipe(format='svg')))\n",
    "   \n",
    "    return estimator\n",
    "\n",
    "inter=interactive(plot_tree \n",
    "   , crit = [\"gini\", \"entropy\"]\n",
    "   , split = [\"best\", \"random\"]\n",
    "   , depth=(2,10)\n",
    "   , min_split=(2,10)\n",
    "   , min_leaf=(1,10))\n",
    "\n",
    "display(inter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"I.5\"></a>\n",
    "### I.5 Advantages and Disadvantages of CART\n",
    "\n",
    "Following are the advantages of decision trees: \n",
    "\n",
    "- Easy to use and understand. \n",
    "- Can handle both categorical and numerical data. \n",
    "- Resistant to outliers, hence require little data preprocessing. \n",
    "- New features can be easily added. \n",
    "- Can be used to build larger classifiers by using ensemble methods (RandomForest).\n",
    "\n",
    "Following are the disadvantages of decision trees: \n",
    "\n",
    "- Prone to overfitting: the algorithm used internaly is splitting on attributes until either it classifies all the data points or there are no more attributes to splits on. As a result, it is prone to creating decision trees that overfit by performing really well on the training data at the expense of accuracy with respect to the entire distribution of data. One way to avoid that is to prevent the tree from growing too deep by stopping it before it perfectly classifies the training data.<br>\n",
    "The __maximum tree depth__ or __minimum number of samples__ required in a split are 2 parameters that can be controlled when the model is instantiated. \n",
    "- Require some kind of measurement as to how well they are doing. \n",
    "- Need to be careful with parameter tuning. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
