{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)\n",
    "## <a href=\"#I\">I Simple SVM</a>\n",
    "## <a href=\"#II\">II Implementing SVM with Scikit-Learn</a>\n",
    "### <a href=\"#II.1\">II.1 Preparing the Data</a>\n",
    "### <a href=\"#II.2\">II.2 Training the Algorithm</a>\n",
    "### <a href=\"#II.3\">II.3 Making Predictions</a>\n",
    "### <a href=\"#II.4\">II.4 Evaluating the algorithm</a>\n",
    "## <a href=\"#III\">III Other Kernel SVM</a>\n",
    "### <a href=\"#III.1\">III.1 Preparing the Data</a>\n",
    "### <a href=\"#III.2\">III.2 Training the Algorithm and making predictions</a>\n",
    "### <a href=\"#III.3\">III.3 Evaluating the algorithm</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)\n",
    "\n",
    "A __support vector machine__ (__SVM__) is a type of supervised machine learning classification algorithm. \n",
    "\n",
    "<a id=\"I\"></a>\n",
    "## I Simple SVM\n",
    "\n",
    "In case of linearly separable data in two dimensions a typical machine learning algorithm tries to find a boundary that divides the data in such a way that the misclassification error can be minimized. \n",
    "In the picture below you can see that there can be several boundaries (3 lines here) that correctly divide the data points. \n",
    "\n",
    "<img src=\"nbimages/svm1.jpg\" width=300 height=300/>\n",
    "\n",
    "SVM differs from the other classification algorithms in the way that it chooses the decision boundary that maximizes the distance from the nearest data points of all the classes: the SVM finds the most optimal decision boundary.\n",
    "\n",
    "The most optimal decision boundary is the one which has maximum margin from the nearest points of all the classes. \n",
    "The nearest points from the _decision boundary_ that maximize the distance between the decision boundary and the points are called __support vectors__. \n",
    "The _decision boundary_ in case of support vector machines is called the __maximum margin classifier__, or the __maximum margin hyper plane__.\n",
    "\n",
    "<img src=\"nbimages/svm2.jpg\" width=300 height=300/>\n",
    "\n",
    "There is complex mathematics involved behind finding the support vectors, calculating the margin between decision boundary and the support vectors and maximizing this margin. \n",
    "\n",
    "<a id=\"II\"></a>\n",
    "## II Implementing SVM with Scikit-Learn\n",
    "\n",
    "Our task is to predict whether a bank currency note is authentic or not based upon four attributes of the note i.e. skewness of the wavelet transformed image, variance of the image, entropy of the image, and curtosis of the image. This is a __binary classification__ problem?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variance</th>\n",
       "      <th>Skewness</th>\n",
       "      <th>Curtosis</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.6661</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.1674</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.6383</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.5228</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.4552</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Variance  Skewness  Curtosis  Entropy  Class\n",
       "0   3.62160    8.6661   -2.8073 -0.44699      0\n",
       "1   4.54590    8.1674   -2.4586 -1.46210      0\n",
       "2   3.86600   -2.6383    1.9242  0.10645      0\n",
       "3   3.45660    9.5228   -4.0112 -3.59440      0\n",
       "4   0.32924   -4.4552    4.5718 -0.98880      0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1372 entries, 0 to 1371\n",
      "Data columns (total 5 columns):\n",
      "Variance    1372 non-null float64\n",
      "Skewness    1372 non-null float64\n",
      "Curtosis    1372 non-null float64\n",
      "Entropy     1372 non-null float64\n",
      "Class       1372 non-null int64\n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 53.7 KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "dataset = pd.read_csv('datasets/bill_authentication.csv')\n",
    "dataset.head()\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"II.1\"></a>\n",
    "### II.1 Preparing the Data\n",
    "\n",
    "Preparing the data involves:\n",
    "\n",
    "1. Dividing the data into attributes and labels \n",
    "2. Dividing the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variance</th>\n",
       "      <th>Skewness</th>\n",
       "      <th>Curtosis</th>\n",
       "      <th>Entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1326</td>\n",
       "      <td>-1.29430</td>\n",
       "      <td>2.6735</td>\n",
       "      <td>-0.84085</td>\n",
       "      <td>-2.03230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1109</td>\n",
       "      <td>-0.40857</td>\n",
       "      <td>3.0977</td>\n",
       "      <td>-2.96070</td>\n",
       "      <td>-2.68920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1139</td>\n",
       "      <td>-1.52280</td>\n",
       "      <td>-6.4789</td>\n",
       "      <td>5.75680</td>\n",
       "      <td>0.87325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>657</td>\n",
       "      <td>-0.27800</td>\n",
       "      <td>8.1881</td>\n",
       "      <td>-3.13380</td>\n",
       "      <td>-2.52760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>704</td>\n",
       "      <td>3.70220</td>\n",
       "      <td>6.9942</td>\n",
       "      <td>-1.85110</td>\n",
       "      <td>-0.12889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Variance  Skewness  Curtosis  Entropy\n",
       "1326  -1.29430    2.6735  -0.84085 -2.03230\n",
       "1109  -0.40857    3.0977  -2.96070 -2.68920\n",
       "1139  -1.52280   -6.4789   5.75680  0.87325\n",
       "657   -0.27800    8.1881  -3.13380 -2.52760\n",
       "704    3.70220    6.9942  -1.85110 -0.12889"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1326    1\n",
       "1109    1\n",
       "1139    1\n",
       "657     0\n",
       "704     0\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dataset.iloc[:, :-1]\n",
    "y = dataset.iloc[:, -1]\n",
    "type(X)\n",
    "type(y)\n",
    "y.head()\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train.head()\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"II.2\"></a>\n",
    "### II.2 Training the Algorithm\n",
    "\n",
    "We have divided the data into training and testing sets. Now is the time to train our SVM on the training data.\n",
    "The SVM class constructor takes, as parameter, a kernel type. \n",
    "In the case of a simple SVM we set this parameter as \"__linear__\" (simple SVMs can only classify linearly separable data). \n",
    "We will see non-linear kernels (\"__rbf__\",\"__polynomial__\") in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"II.3\"></a>\n",
    "### II.3 Making Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svclassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"II.4\"></a>\n",
    "### II.4 Evaluating the Algorithm\n",
    "\n",
    "__Confusion matrix__, __precision__, __recall__, and __F1 measures__ are the most commonly used metrics for classification tasks. \n",
    "Here is the code for finding these metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[155   2]\n",
      " [  0 118]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       157\n",
      "           1       0.98      1.00      0.99       118\n",
      "\n",
      "    accuracy                           0.99       275\n",
      "   macro avg       0.99      0.99      0.99       275\n",
      "weighted avg       0.99      0.99      0.99       275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results it can be observed that there is only one misclassification: a very good result !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"III\"></a>\n",
    "## III Other Kernel SVM\n",
    "\n",
    "The simple SVM algorithm (the one using the kernel __linear__) can be used to find decision boundary for linearly separable data. \n",
    "In the case of non-linearly separable data, such as the one shown belo, a straight line cannot be used as a decision boundary. A different SVM Kernel is used.\n",
    "\n",
    "<img src=\"nbimages/svm3.jpg\" width=300 height=300/>\n",
    "\n",
    "Basically, with a non linear kernel, the SVM algorithm projects the non-linearly separable data lower dimensions to linearly separable data in higher dimensions in such a way that data points belonging to different classes are allocated to different dimensions. \n",
    "Again, there is complex mathematics behind this algorithm.\n",
    "\n",
    "In this example we will try to predict the category to which a plant belongs based on four attributes: _sepal-width_, _sepal-length_, _petal-width_ and _petal-length_.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal-length</th>\n",
       "      <th>sepal-width</th>\n",
       "      <th>petal-length</th>\n",
       "      <th>petal-width</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal-length  sepal-width  petal-length  petal-width        Class\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colnames = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']\n",
    "dataset2 = pd.read_csv('datasets/iris.data', names=colnames)\n",
    "dataset2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"III.1\"></a>\n",
    "### III.1 Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset2.drop('Class', axis=1)\n",
    "y = dataset2['Class']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"III.2\"></a>\n",
    "### III.2 Training the Algorithm and making predictions\n",
    "\n",
    "In the case of the simple SVM we used \"__linear__\" as the value for the kernel parameter.<br> \n",
    "In this situation, we are dealing with non-linearly separable data, we should use a different SVM kernel: __Radial basis function__ (__rbf__), __polynomial__ (__poly__) or __sigmoid__. <br>\n",
    "\n",
    "__Note__: the parameter __degree__ represent the degree of the polynomial kernel function (it is ignored by all other kernels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3917a9bd1144fa69c3244ea9cd50924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='kernelType', options=('poly', 'rbf', 'sigmoid'), value='poly'), Inâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display                               \n",
    "from ipywidgets import interactive\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def testKernel(kernelType, degree):\n",
    "    svclassifier = SVC(kernel=kernelType, degree=degree, gamma='scale')\n",
    "    svclassifier.fit(X_train, y_train)\n",
    "    y_pred = svclassifier.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(classification_report(y_test,y_pred))\n",
    "\n",
    "inter=interactive(testKernel \n",
    "   , kernelType = ['poly', 'rbf', 'sigmoid']\n",
    "   , degree = (1,20))\n",
    "\n",
    "display(inter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"III.3\"></a>\n",
    "### III.3 Evaluating the Algorithm\n",
    "\n",
    "If we compare the performance of the different types of kernels we can clearly see that the __sigmoid__ kernel performs the worst. This is due to the reason that sigmoid function returns two values, 0 and 1, therefore it is more suitable for binary classification problems. However, in our case we had three output classes !\n",
    "\n",
    "Amongst the __Radial basis function__ kernel and __polynomial__ kernel, we can see that Gaussian kernel achieved a perfect 100% prediction rate while polynomial kernel misclassified one instance. Therefore the Gaussian kernel performed slightly better. <br>\n",
    "However, there is no hard and fast rule as to which kernel performs best in every scenario. It is all about testing all the kernels and selecting the one with the best results on your test dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
